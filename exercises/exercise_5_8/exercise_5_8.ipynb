{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](exercise_5_8_1.png)\n",
    "![](exercise_5_8_2.png)\n",
    "![](off_policy_mc_control.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_up(dict_):\n",
    "    def look_up_(x):\n",
    "        return dict_[x]\n",
    "    return look_up_\n",
    "\n",
    "z = look_up({1:2, 2: 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_mc_control():\n",
    "    \n",
    "    action_value = {}\n",
    "    normalizing_constant = {}\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        behaviour_policy = epsilon_greedy_policy(target_policy, EPSILON)\n",
    "        episode = run_episode(behaviour_policy, state_0)\n",
    "        episode = list(reversed(x))\n",
    "        \n",
    "        G = 0\n",
    "        W = 1\n",
    "        \n",
    "        # Iterate in tuples (t + 1, t)\n",
    "        for new_state, new_action, new_reward, state, action, reward in zip(episode, episode[1:]):\n",
    "            \n",
    "            G = GAMMA * G + new_reward\n",
    "            \n",
    "\n",
    "            normalizing_constant[(state, action)] = normalizing_constant.get((state, action), 0) + W\n",
    "            \n",
    "            action_value[(state, action)] = (\n",
    "                action_value.get((state, action), 0) +\n",
    "                W / normalizing_constant.get((state, action), 0) * (G - action_value.get((state, action), 0))\n",
    "            )\n",
    "            \n",
    "            target_policy[state] = best_action(state, action_value)\n",
    "            \n",
    "            if action != target_policy[state]:\n",
    "                break\n",
    "                \n",
    "            W = W / behaviour_policy(state, action)\n",
    "\n",
    "\n",
    "def run_episode(behaviour_policy, state_0):\n",
    "    \n",
    "    state = state_0.copy()\n",
    "    episode = [(state, 0, -1)]\n",
    "    \n",
    "    while cell_type(state) != 'GOAL':\n",
    "        \n",
    "        action = behaviour_policy(state)\n",
    "        \n",
    "        state = move_car(state, action)\n",
    "        \n",
    "        episode.append(\n",
    "            (state, action, -1)\n",
    "        )\n",
    "    \n",
    "    return episode\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(target_policy, epsilon = 0.1):\n",
    "    def policy(state):\n",
    "        if random.random() <= epsilon:\n",
    "            return random.choice(actions(state))\n",
    "        return target_policy[state]\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.678572456155053"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "\n",
    "MAP = pd.read_csv('maps/map1.csv', header=None).values\n",
    "CELL_TYPE_MAP = {\n",
    "    0: 'WALL',\n",
    "    1: 'TRACK',\n",
    "    2: 'GOAL',\n",
    "    3: 'START',\n",
    "}\n",
    "\n",
    "EPSILON = 0.1\n",
    "\n",
    "def states():\n",
    "    \"\"\"Generates all possible states.\n",
    "    \"\"\"\n",
    "    for x, y in product(range(MAP.shape[0]), range(MAP.shape[1])):\n",
    "        yield (x, y)\n",
    "        \n",
    "def cell_type(x, y):\n",
    "    return CELL_TYPE_MAP[MAP[x, y]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actions(state):\n",
    "    \"\"\"Generate all velocity vectors at a given state.\n",
    "    \"\"\"\n",
    "    _, _, dx, dy = state\n",
    "    \n",
    "    for ddx in [-1, 0, 1]:\n",
    "        for ddy in [-1, 0, 1]:\n",
    "            \n",
    "            new_dx = dx + ddx\n",
    "            new_dy = dy + ddy\n",
    "            \n",
    "            \n",
    "            # Both velocity components are restricted to be positive and less than 5.\n",
    "            if not (0 <= new_dx <= 5 and 0 <= new_dy <= 5):\n",
    "                continue\n",
    "            \n",
    "            yield (new_dx, new_dy)\n",
    "            \n",
    "\n",
    "def move_car(state, action):\n",
    "    \"\"\"Move a car using the new velocity vector.\n",
    "    \"\"\"\n",
    "    x, y, dx, dy = state\n",
    "    ddx, ddy = action\n",
    "    \n",
    "    new_dx = dx + ddx\n",
    "    new_dy = dy + ddy\n",
    "    \n",
    "    new_x = x + new_dx\n",
    "    new_y = y + new_dy\n",
    "    \n",
    "    return (new_x, new_y, new_dx, new_dy)\n",
    "            \n",
    "assert len(list(actions((0, 0, 1, 1)))) == 9, 'total of 9 actions.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2],\n",
       " [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2],\n",
       " [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2],\n",
       " [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
