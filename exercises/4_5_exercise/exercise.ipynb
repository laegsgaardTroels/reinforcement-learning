{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "**Example 4.2**: Jack’s Car Rental Jack manages two locations for a nationwide car rental company.\n",
    "Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he\n",
    "rents it out and is credited \\\\$10 by the national company. If he is out of cars at that location, then the\n",
    "business is lost. Cars become available for renting the day after they are returned. To help ensure that\n",
    "cars are available where they are needed, Jack can move them between the two locations overnight, at\n",
    "a cost of \\\\$2 per car moved. We assume that the number of cars requested and returned at each location\n",
    "$n$\n",
    "are Poisson random variables, meaning that the probability that the number is $n$ is $\\frac{\\lambda^n}{n!} e^{-\\lambda}$ , where $\\lambda$ is\n",
    "the expected number. Suppose $\\lambda$ is 3 and 4 for rental requests at the first and second locations and\n",
    "3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20\n",
    "cars at each location (any additional cars are returned to the nationwide company, and thus disappear\n",
    "from the problem) and a maximum of five cars can be moved from one location to the other in one\n",
    "night. We take the discount rate to be $\\gamma = 0.9$ and formulate this as a continuing finite MDP, where\n",
    "the time steps are days, the state is the number of cars at each location at the end of the day, and\n",
    "the actions are the net numbers of cars moved between the two locations overnight. Figure 4.2 shows the sequence of policies found by policy iteration starting from the policy that never moves any cars.\n",
    "\n",
    "![](jacks_car_rental.png)\n",
    "\n",
    "**Figure 4.2**: The sequence of policies found by policy iteration on Jack’s car rental problem, and the final\n",
    "state-value function. The first five diagrams show, for each number of cars at each location at the end of the\n",
    "day, the number of cars to be moved from the first location to the second (negative numbers indicate transfers\n",
    "from the second location to the first). Each successive policy is a strict improvement over the previous policy,\n",
    "and the last policy is optimal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "![](policy_iteration.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from itertools import product\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "from functools import lru_cache\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "\n",
    "\n",
    "LAMBDA_1_REQUEST = 3\n",
    "LAMBDA_2_REQUEST = 4\n",
    "LAMBDA_1_RETURN = 3\n",
    "LAMBDA_2_RETURN = 2\n",
    "\n",
    "MAX_CARS_AT_LOC = 20\n",
    "MAX_MOVES_OVER_NIGHT = 5\n",
    "\n",
    "def states():\n",
    "    \"\"\"Generates all possible states.\n",
    "    \"\"\"\n",
    "    for location_1, location_2 in product(range(21), range(21)):\n",
    "        yield (location_1, location_2)\n",
    "\n",
    "STATE_SPACE_CARDINALITY = len(list(states()))\n",
    "\n",
    "THETA = 1e-09\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration():\n",
    "    \"\"\"Do the three steps in policy iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Initialize.\n",
    "    state_value = {s: 0 for s in states()}\n",
    "    policy = {s: 0 for s in states()}\n",
    "    \n",
    "    # 2. Policy Evaluation.\n",
    "    state_value = policy_evaluation(policy, state_value)\n",
    "    \n",
    "    # 3. Policy Improvement.\n",
    "    policy_improvement_iteration = 1\n",
    "    while True:\n",
    "        \n",
    "        old_policy = policy.copy()\n",
    "        policy_stable = True\n",
    "        \n",
    "        for idx, state in enumerate(states()):\n",
    "\n",
    "            policy[state] = best_action(state, state_value)\n",
    "            \n",
    "            if policy[state] != old_policy[state]:\n",
    "                policy_stable = False\n",
    "            \n",
    "        if policy_stable:\n",
    "            break\n",
    "        \n",
    "        state_value = policy_evaluation(policy, state_value, policy_improvement_iteration)\n",
    "\n",
    "        policy_improvement_iteration += 1\n",
    "\n",
    "    return policy, state_value\n",
    "\n",
    "\n",
    "def best_action(state, state_value):\n",
    "    state_actions = np.array(list(actions(state)))\n",
    "    return state_actions[np.argmax([\n",
    "        action_value(state, action, state_value) for action in actions(state)\n",
    "    ])]\n",
    "\n",
    "\n",
    "def policy_evaluation(policy, state_value, policy_improvement_iteration=0):\n",
    "    \"\"\"Evaluate a given policy.\n",
    "    \"\"\"\n",
    "    policy_evaluation_iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        old_state_value = state_value.copy()\n",
    "\n",
    "        delta = 0\n",
    "        \n",
    "        for state_progress, state in enumerate(states()):\n",
    "\n",
    "            action = policy[state]\n",
    "            \n",
    "            state_value[state] = action_value(state, action, state_value)\n",
    "            \n",
    "            delta = max(delta, abs(old_state_value[state] - state_value[state]))\n",
    "            \n",
    "            \n",
    "            track_progress(policy_improvement_iteration, policy_evaluation_iteration, state_progress, delta)\n",
    "        \n",
    "        \n",
    "        if delta < THETA:\n",
    "            break\n",
    "            \n",
    "        policy_evaluation_iteration += 1\n",
    "\n",
    "    return state_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_progress(policy_improvement_iteration, policy_evaluation_iteration, state_progress, delta):\n",
    "    # Used to track progress.\n",
    "    clear_output(wait = True)\n",
    "    policy_evaluation_progress = str(round(100 * (state_progress + 1.0) / STATE_SPACE_CARDINALITY, 4))\n",
    "    policy_evaluation_progress = policy_evaluation_progress + ('0' * (6 - len(policy_evaluation_progress)))  # Add padding.\n",
    "    print(\n",
    "        f'Policy improvement iteration {policy_improvement_iteration} \\n'\n",
    "        f'Policy evaluation iteration {policy_evaluation_iteration} \\n'\n",
    "        f'Policy evaluation progress: {policy_evaluation_progress} % \\n'\n",
    "        f'Policy evaluation delta: {delta}'\n",
    "    )\n",
    "\n",
    "\n",
    "def action_value(state, action, state_value = None):\n",
    "    \"\"\"Action value function.\n",
    "    \"\"\"    \n",
    "    \n",
    "    if action != 0:\n",
    "        moved_cars_state = move_cars(state, action)\n",
    "        return action_value(moved_cars_state, action=0, state_value = state_value) - 2 * abs(action)\n",
    "\n",
    "    sigma = 0\n",
    "    total_probability = 0\n",
    "    \n",
    "    for requests in possible_requests(state):\n",
    "\n",
    "        requested_cars_state = request_cars(state, requests)\n",
    "\n",
    "        for returns in possible_returns(requested_cars_state):\n",
    "\n",
    "            returned_cars_state = return_cars(requested_cars_state, returns)\n",
    "            \n",
    "            probability = probability_(requests, returns)\n",
    "\n",
    "            sigma += probability * (10 * sum(requests) + GAMMA * state_value[returned_cars_state])\n",
    "            total_probability += probability\n",
    "    \n",
    "    # Conditional probability\n",
    "    # p(new_state, reward | state, action) = p(new_state, reward, state, action) / p(state, action)\n",
    "    #                                      = p(new_state, reward, state, action) / total_probability\n",
    "    return sigma / total_probability\n",
    "    \n",
    "\n",
    "def possible_requests(state):\n",
    "    \"\"\"Generates possible requests at a given state.\n",
    "    \"\"\"\n",
    "    for requests in product(range(state[0] + 1), range(state[1] + 1)):\n",
    "        if requests_probability(requests) > 1e-5:\n",
    "            yield requests\n",
    "\n",
    "\n",
    "def possible_returns(state):\n",
    "    \"\"\"Generates possible returns at a given state.\n",
    "    \"\"\"\n",
    "    for returns in product(range(MAX_CARS_AT_LOC  - state[0] + 1), range(MAX_CARS_AT_LOC  - state[1] + 1)):\n",
    "        if returns_probability(returns) > 1e-5:\n",
    "            yield returns\n",
    "\n",
    "\n",
    "def move_cars(state, action):\n",
    "    \"\"\"State after cars has been moved.\n",
    "    \"\"\"\n",
    "    return (state[0] - action, state[1] + action)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def request_cars(state, requests):\n",
    "    \"\"\"State after cars has been requested.\n",
    "    \"\"\"\n",
    "    return (state[0] - requests[0], state[1] - requests[1])\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def return_cars(state, returns):\n",
    "    \"\"\"State after cars has been returned.\n",
    "    \"\"\"\n",
    "    return (state[0] + returns[0], state[1] + returns[1])\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def probability_(requests, returns):\n",
    "    \"\"\"Probability of given requests and returns.\n",
    "    \"\"\"\n",
    "    return requests_probability(requests) * returns_probability(returns)\n",
    "\n",
    "\n",
    "def requests_probability(requests):\n",
    "    return poisson_probability(LAMBDA_1_REQUEST, requests[0]) * poisson_probability(LAMBDA_1_REQUEST, requests[1])\n",
    "\n",
    "\n",
    "def returns_probability(returns):\n",
    "    return poisson_probability(LAMBDA_1_RETURN, returns[0]) * poisson_probability(LAMBDA_2_RETURN, returns[1])\n",
    "\n",
    "\n",
    "def actions(state):\n",
    "    \"\"\"Generates the actions available at a given state.\n",
    "    \"\"\"\n",
    "    for action in range(-MAX_MOVES_OVER_NIGHT, MAX_MOVES_OVER_NIGHT + 1):\n",
    "        \n",
    "        # More cars moved from location 2 than cars at location 2.\n",
    "        if action < - state[1]:\n",
    "            continue\n",
    "        \n",
    "        # More cars moved from location 1 than cars at location 1.\n",
    "        if action > state[0]:\n",
    "            continue\n",
    "        \n",
    "        # More cars moved from location 2 than space available at location 1.\n",
    "        if action < - (MAX_CARS_AT_LOC - state[0]):\n",
    "            continue\n",
    "            \n",
    "        # More cars moved from location 2 than space available at location 1.\n",
    "        if action > MAX_CARS_AT_LOC - state[1]:\n",
    "            continue\n",
    "            \n",
    "        yield action\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def reward(requests, action):\n",
    "    \"\"\"NOT USED.\n",
    "    \"\"\"\n",
    "    return 10 * sum(requests) - 2 * abs(action)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def poisson_probability(lambda_, n):\n",
    "    \"\"\"Probability mass function of the possion distribution.\n",
    "    \"\"\"\n",
    "    return (lambda_ ** n / np.math.factorial(n)) * np.exp(-lambda_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "assert best_action(state=(0,0), state_value={state: 0 if state == (0,0) else -10 for state in states()}) == 0, (\n",
    "    \"Any other state value is worse than the current, hence no action should be taken.\"\n",
    ")\n",
    "\n",
    "assert (list(actions((1, 2))) == [-2, -1, 0, 1])\n",
    "assert (list(actions((2, 1))) == [-1, 0, 1, 2])\n",
    "assert (list(actions((20, 19))) == [0, 1])\n",
    "\n",
    "for requests in possible_requests((0, 0)):\n",
    "    probability = requests_probability(requests)\n",
    "    assert requests_probability(requests) > 0, 'Probability should be positive.'\n",
    "    \n",
    "\n",
    "state = random.choice(list(states()))\n",
    "for requests in possible_requests(state):\n",
    "\n",
    "        requested_cars_state = request_cars(state, requests)\n",
    "        assert requested_cars_state in list(states()), (\n",
    "            'Not a valid state after request, '\n",
    "            f'state: {state}, requests: {requests}, requested_cars_state: {requested_cars_state}.'\n",
    "        )\n",
    "\n",
    "        for returns in possible_returns(requested_cars_state):\n",
    "\n",
    "            returned_cars_state = return_cars(requested_cars_state, returns)\n",
    "            assert returned_cars_state in list(states()), (\n",
    "                'Not a valid state after returns, '\n",
    "                f'state: {state}, returned_cars_state: {returned_cars_state}, returns: {returns}.'\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy improvement iteration 0 \n",
      "Policy evaluation iteration 90 \n",
      "Policy evaluation progress: 24.263 % \n",
      "Policy evaluation delta: 2.144761083400226e-06\n"
     ]
    }
   ],
   "source": [
    "policy, state_value = policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('policy.json', 'wb') as fp:\n",
    "    json.dump(policy, fp, indent=4)\n",
    "\n",
    "with open('state_value.json', 'wb') as fp:\n",
    "    json.dump(state_value, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('policy.json', 'rb') as fp:\n",
    "    policy = json.load(fp)\n",
    "\n",
    "with open('state_value.json', 'rb') as fp:\n",
    "    statet_value = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_matrix(d):\n",
    "    \"\"\"Convert a {(x, y): z} to a matrix with entry [z] in entry x, y.\n",
    "    \"\"\"\n",
    "    matrix = np.zeros((21, 21))\n",
    "    for i in range(21):\n",
    "        for j in range(21):\n",
    "            matrix[i, j] = d[(i, j)]\n",
    "    return matrix\n",
    "\n",
    "def annotated_matrix(ax, matrix):\n",
    "    \"\"\"Plot annotated matrix in axis.\n",
    "    \"\"\"\n",
    "    ax.matshow(matrix, cmap=plt.cm.Blues)\n",
    "    plt.gca().invert_yaxis()\n",
    "    for i in range(21):\n",
    "        for j in range(21):\n",
    "            c = matrix[j, i]\n",
    "            ax.text(i, j, str(int(c)), va='center', ha='center')\n",
    "\n",
    "def plot_surface(ax, d):\n",
    "    \"\"\"Plot a surface of {(x,y): z}\n",
    "    \"\"\"\n",
    "    # Make data.\n",
    "    X = np.array([s[0] for s in d.keys()])\n",
    "    Y = np.array([s[1] for s in d.keys()])\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "    @np.vectorize\n",
    "    def look_up(x, y):\n",
    "        return d[(x, y)]\n",
    "    Z = look_up(X, Y)\n",
    "    \n",
    "    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "    ax.view_init(30, 160)\n",
    "\n",
    "# Make the plots.            \n",
    "            \n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "ax1 = plt.subplot(221)\n",
    "annotated_matrix(\n",
    "    ax1,\n",
    "    dict_to_matrix(policy)\n",
    ")\n",
    "_ = ax1.set_title('Policy Function.')\n",
    "\n",
    "\n",
    "ax2 = plt.subplot(222)\n",
    "annotated_matrix(\n",
    "    ax2,\n",
    "    dict_to_matrix(state_value)\n",
    ")\n",
    "_ = ax2.set_title('State Value Function.')\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(223, projection='3d')\n",
    "plot_surface(\n",
    "    ax3,\n",
    "    state_value\n",
    ")\n",
    "_ = ax3.set_title('State Value Function.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "def profile():\n",
    "    state_value = {s: 0 for s in STATES}\n",
    "    policy = {s: 0 for s in STATES}\n",
    "    for state_progress, state in enumerate(STATES):\n",
    "            action = policy[state]\n",
    "            state_value[state] = action_value(state, 0, state_value)\n",
    "\n",
    "cProfile.run('profile()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.5 (programming)\n",
    "Write a program for policy iteration and re-solve Jack’s car rental problem with the following changes. One of Jack’s employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs \\\\$2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), then an additional cost of $4 must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than dynamic programming. To check your program, first replicate the results given for the original problem. If your computer is too slow for the full problem, cut all the numbers of cars in half."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
