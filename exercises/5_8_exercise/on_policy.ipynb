{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](exercise_5_8_1.png)\n",
    "![](exercise_5_8_2.png)\n",
    "![](on_policy_algorithm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import clear_output\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import random\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from math import sin, pi\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "MAP = pd.read_csv('maps/map1.csv', header=None).values\n",
    "CELL_TYPE_MAP = {\n",
    "    0: 'WALL',\n",
    "    1: 'TRACK',\n",
    "    2: 'GOAL',\n",
    "    3: 'START',\n",
    "}\n",
    "\n",
    "# NB: MAP is row, columns, but we want (x, y) which is the reverse.\n",
    "MAX_Y = MAP.shape[0] - 1\n",
    "MAX_X = MAP.shape[1] - 1\n",
    "MAX_VELOCITY = 5\n",
    "\n",
    "GAMMA = 0.8\n",
    "EPSILON = 0.01\n",
    "MAX_ITERATIONS = 10000\n",
    "\n",
    "CHEAT_CODES = False\n",
    "\n",
    "\n",
    "def states():\n",
    "    \"\"\"Generates all possible states.\n",
    "    \"\"\"\n",
    "    for x, y, dx, dy in product(\n",
    "        range(MAX_X + 1), range(MAX_Y + 1), range(0, MAX_VELOCITY + 1), range(0, MAX_VELOCITY + 1)\n",
    "    ):\n",
    "        yield (x, y, dx, dy)\n",
    "        \n",
    "\n",
    "def cell_type(state):\n",
    "    \"\"\"Cell types is given by values of CELL_TYPE_MAP.\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y, dx, dy = state\n",
    "    return CELL_TYPE_MAP[MAP[MAX_Y - y, x]]\n",
    "\n",
    "        \n",
    "START_STATES = list(set(map(\n",
    "    lambda state: (state[0], state[1], 0, 0),\n",
    "    filter(\n",
    "        lambda state: cell_type(state) == 'START', \n",
    "        states()\n",
    "    )\n",
    ")))\n",
    "\n",
    "        \n",
    "def is_valid_state(state):\n",
    "    x, y, dx, dy = state\n",
    "    valid_x = (0 <= x <= MAX_X)\n",
    "    valid_y = (0 <= y <= MAX_Y)\n",
    "    valid_dx = (0 <= dx <= MAX_VELOCITY)\n",
    "    valid_dy = (0 <= dy <= MAX_VELOCITY)\n",
    "    return valid_x and valid_y and valid_dx and valid_dy\n",
    "\n",
    "\n",
    "def actions(state):\n",
    "    \"\"\"Generate all velocity vectors at a given state.\n",
    "    \"\"\"\n",
    "    assert(is_valid_state(state))\n",
    "    for ddx in [-1, 0, 1]:\n",
    "        for ddy in [-1, 0, 1]:\n",
    "            \n",
    "            action = (ddx, ddy)\n",
    "            new_state = move_car(state, action)\n",
    "            \n",
    "            if not is_valid_state(new_state):\n",
    "                continue\n",
    "                \n",
    "            if cell_type(new_state) == 'WALL' and CHEAT_CODES:\n",
    "                continue\n",
    "             \n",
    "            yield (ddx, ddy)\n",
    "            \n",
    "\n",
    "def move_car(state, action):\n",
    "    \"\"\"Move a car using the velocity vector.\n",
    "    \"\"\"\n",
    "    assert(is_valid_state(state))\n",
    "    x, y, dx, dy = state\n",
    "    ddx, ddy = action\n",
    "    \n",
    "    new_dx = dx + ddx\n",
    "    new_dy = dy + ddy\n",
    "    \n",
    "    new_x = x + new_dx\n",
    "    new_y = y + new_dy\n",
    "    \n",
    "    new_state = (new_x, new_y, new_dx, new_dy)\n",
    "\n",
    "    return new_state\n",
    "  \n",
    "\n",
    "class epsilon_greedy_policy:\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        self.policy = {}\n",
    "\n",
    "    def __setitem__(self, state, action):\n",
    "        self.policy[state] = action\n",
    "\n",
    "    def __getitem__(self, state):\n",
    "        random_state = random.choice(list(actions(state)))\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random_state\n",
    "        return self.policy.get(state, random_state)\n",
    "    \n",
    "\n",
    "class epsilon_greedy_policy_sinus:\n",
    "    def __init__(self, epsilon_rotation, max_epsilon = 0.05):\n",
    "        \n",
    "        self.epsilon_rotation = epsilon_rotation\n",
    "        self.max_epsilon = max(self.epsilon_rotation, max_epsilon)\n",
    "        \n",
    "        self.epsilon = 1.0\n",
    "        self.policy = {}\n",
    "        self.counter = 1.0\n",
    "        self.clock_speed = 500\n",
    "        \n",
    "    def tick(self):\n",
    "        self.counter += 1.0\n",
    "        self.epsilon = (\n",
    "            self.epsilon_rotation * sin(self.clock_speed * self.counter) + \n",
    "            (- ((self.max_epsilon - self.epsilon_rotation) - self.epsilon_rotation) / MAX_ITERATIONS) * self.counter \n",
    "            + (self.max_epsilon - self.epsilon_rotation)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def __setitem__(self, state, action):\n",
    "        self.policy[state] = action\n",
    "\n",
    "    def __getitem__(self, state):\n",
    "        random_state = random.choice(list(actions(state)))\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random_state\n",
    "        return self.policy.get(state, random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_progress(iteration, episode_length, epsilon):\n",
    "    clear_output(wait = True)\n",
    "    print(\n",
    "        f'Episode number: {iteration}\\n'\n",
    "        f'Episode length: {episode_length}\\n'\n",
    "        f'Epsilon: {epsilon}'\n",
    "    )\n",
    "\n",
    "def on_policy():\n",
    "    \n",
    "    action_value = {}\n",
    "    visit_count = {}\n",
    "    epsilon_soft_policy = epsilon_greedy_policy_sinus(EPSILON)\n",
    "\n",
    "    # Used to track progress.\n",
    "    iteration = 1\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        start_state = random.choice(START_STATES)\n",
    "        episode = run_episode(epsilon_soft_policy, start_state)\n",
    "        \n",
    "        G = 0\n",
    "        visited_states = []\n",
    "        \n",
    "        # Used to track progress.\n",
    "        episode_length = 1\n",
    "        \n",
    "        for state, action, reward in reversed(episode):\n",
    "            \n",
    "            G = G + GAMMA * reward\n",
    "            \n",
    "            if (state, action,) in visited_states:\n",
    "                continue\n",
    "            \n",
    "            visit_count[(state, action)] = visit_count.get((state, action), 0) + 1\n",
    "\n",
    "            action_value[(state, action)] = (\n",
    "                action_value.get((state, action), -10) +\n",
    "                1 / visit_count[(state, action)] * (G - action_value.get((state, action), -10))\n",
    "            )\n",
    "            \n",
    "            epsilon_soft_policy[state] = best_action(state, action_value)\n",
    "            \n",
    "            visited_states.append(\n",
    "                (state, action,)\n",
    "            )\n",
    "            \n",
    "            episode_length += 1\n",
    "            \n",
    "        if 'tick' in dir(epsilon_soft_policy):\n",
    "            epsilon_soft_policy.tick()\n",
    "            \n",
    "        track_progress(iteration, episode_length, epsilon_soft_policy.epsilon)\n",
    "        iteration += 1\n",
    "        \n",
    "        if iteration > MAX_ITERATIONS:\n",
    "            break\n",
    "    \n",
    "    return epsilon_soft_policy, action_value\n",
    "\n",
    "def run_episode(behaviour_policy, start_state):\n",
    "    \"\"\"Used to generate an episode.\n",
    "    \n",
    "    Remember:\n",
    "        state -> action -> reward -> ...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize episode at start state, the 0 action, and 0 reward.\n",
    "    state = start_state\n",
    "    episode = []\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        action = behaviour_policy[state]\n",
    "        \n",
    "        episode.append(\n",
    "            (state, action, -1)\n",
    "        )\n",
    "        \n",
    "        state = move_car(state, action)\n",
    "        \n",
    "        \n",
    "        # If no actions are available from state, then reset.\n",
    "        if len(list(actions(state))) == 0:\n",
    "            state = random.choice(START_STATES)\n",
    "            continue\n",
    "            \n",
    "        # If state is invalid or a wall, then reset.\n",
    "        if not is_valid_state(state) or cell_type(state) == 'WALL':\n",
    "            state = random.choice(START_STATES)\n",
    "            continue\n",
    "            \n",
    "        if cell_type(state) == 'GOAL':\n",
    "            episode.append(\n",
    "                (state, (-10, -10), -1)\n",
    "            )\n",
    "            break\n",
    "            \n",
    "    return episode\n",
    "\n",
    "\n",
    "def best_action(state, action_value):\n",
    "    \"\"\"Returns the best action in a state using the action values.\n",
    "    \"\"\"\n",
    "    state_action_value = [(action_value.get((state, action), -10), action) for action in actions(state)]\n",
    "    best_action = max(state_action_value)[1]\n",
    "    \n",
    "    assert isinstance(best_action, tuple), f'best_action: {best_action}'\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run episodes from all starting points.\n",
    "for start_state in START_STATES:\n",
    "    epsilon_soft_policy = epsilon_greedy_policy(EPSILON)\n",
    "    episode = run_episode(epsilon_soft_policy, start_state)\n",
    "\n",
    "# For whatever state does shit then make sense.\n",
    "for state in states():\n",
    "    \n",
    "    assert len(list(actions(state))) <= 9, f\"state: {state}, action: {action}, new_state: {new_state}.\"\n",
    "    assert cell_type(state) in CELL_TYPE_MAP.values(), f\"state: {state}, action: {action}, new_state: {new_state}.\"\n",
    "    \n",
    "    for action in actions(state):\n",
    "        \n",
    "        new_state = move_car(state, action)\n",
    "        new_x, new_y, new_dx, new_dy = new_state\n",
    "        \n",
    "        assert 0 <= new_x <= MAX_X, f\"state: {state}, action: {action}, new_state: {new_state}.\"\n",
    "        assert 0 <= new_y <= MAX_Y, f\"state: {state}, action: {action}, new_state: {new_state}.\"\n",
    "        \n",
    "        assert 0 <= new_dx <= MAX_VELOCITY, f\"state: {state}, action: {action}, new_state: {new_state}.\"\n",
    "        assert 0 <= new_dy <= MAX_VELOCITY, f\"state: {state}, action: {action}, new_state: {new_state}.\"\n",
    "        \n",
    "        assert cell_type(new_state) in CELL_TYPE_MAP.values(), (\n",
    "            f\"state: {state}, action: {action}, new_state: {new_state}.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episode(episode):\n",
    "    reconstructed_episode = reconstruct_last_episode(episode)\n",
    "    buffer = 1\n",
    "    ax = plt.axes(xlim=(-buffer, MAX_X + buffer), ylim=(-buffer, MAX_Y + buffer))\n",
    "    for i in range(len(reconstructed_episode)-1):\n",
    "        animate(i, reconstructed_episode)\n",
    "    plot_walls()\n",
    "    \n",
    "    \n",
    "def reconstruct_last_episode(episode):\n",
    "    \"\"\"Reconstuct a path.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    at_starting_state = False\n",
    "    for state_action_reward in reversed(episode):\n",
    "        state, _, _ = state_action_reward\n",
    "        x, y, dx, dy = state\n",
    "        res.append(\n",
    "            [x, y]\n",
    "        )   \n",
    "    \n",
    "    return list(reversed(res))\n",
    "\n",
    "def plot_walls():\n",
    "    \"\"\"Plot the walls in the map.\n",
    "    \"\"\"\n",
    "    for x, y in set(map(\n",
    "            lambda state: (state[0], state[1]),\n",
    "            filter(\n",
    "                lambda state: cell_type(state) == 'WALL',\n",
    "                list(states())\n",
    "            )\n",
    "        )):\n",
    "        plt.plot([x], [y], marker='o', markersize=3, color=\"black\")\n",
    "\n",
    "\n",
    "def animate(i, reconstructed_episode):\n",
    "    \"\"\"Used to animate the plot.\n",
    "    \"\"\"\n",
    "    [[x1, y1], [x2, y2]] = reconstructed_episode[i:int(i+2)]\n",
    "    \n",
    "    plt.plot([x1,x2],[y1,y2], 'r')\n",
    "    \n",
    "    if cell_type((x1, y1, 0, 0)) == 'START':\n",
    "        plt.plot(x1, y1, 'g*')\n",
    "    \n",
    "    if cell_type((x2, y2, 0, 0)) == 'GOAL':\n",
    "        plt.plot(x2, y2, 'r*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the algorithm and analyze output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode number: 10000\n",
      "Episode length: 43\n",
      "Epsilon: 0.019635392624634405\n"
     ]
    }
   ],
   "source": [
    "epsilon_soft_policy, action_value = on_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(map(lambda state: state in epsilon_soft_policy.policy.keys(), START_STATES)), (\n",
    "    'All the start states should be in the keys of the policy.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Race mate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-c04f08f87d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTART_STATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mepisode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon_soft_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maccumulated_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0maccumulated_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-155-1a2a12c74de8>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(behaviour_policy, start_state)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# If no actions are available from state, then reset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTART_STATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-186-245c70a06937>\u001b[0m in \u001b[0;36mactions\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_valid_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mddx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mddy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mddx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_state = random.choice(START_STATES)\n",
    "episode = run_episode(epsilon_soft_policy.policy, start_state)\n",
    "accumulated_reward = 0\n",
    "for state, action, reward in episode:\n",
    "    accumulated_reward += reward\n",
    "    \n",
    "    cell_str = cell_type(state)\n",
    "    state_str = f'state: {state} ' \n",
    "    action_str = f'action: {action} '\n",
    "    reward_str = f'accumulated_reward: {accumulated_reward} '\n",
    "    print(\n",
    "         cell_str + ' ' * ( 6 - len(cell_str)) + ': '+\n",
    "        (state_str + ' ' * (23 - len(state_str))) +\n",
    "        (action_str + ' ' * (23 - len(action_str))) +\n",
    "        (reward_str + ' ' * (25 - len(reward_str)))\n",
    "    )\n",
    "plt.figure()\n",
    "plot_episode(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/troels/git/reinforcement_learning_sutton_project/.venv/lib/python3.7/site-packages/ipykernel_launcher.py:4: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "for start_state in START_STATES:\n",
    "    episode = run_episode(epsilon_soft_policy.policy, start_state)\n",
    "    plot_episode(episode)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
